{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# P1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "dHOjDhpInJhd",
        "outputId": "3668a588-71ae-45ce-c0f3-7fed6c59a462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thiết lập đường dẫn và thư mục hoàn tất.\n",
            "Đang tải dữ liệu từ file .mat...\n",
            "Số lượng bản ghi: 53\n",
            "\n",
            "Bắt đầu Tiền xử lý dữ liệu (KHÔNG chuẩn hóa HR/RR)...\n",
            "\n",
            "Đã xử lý thành công 0/53 bản ghi.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Không có dữ liệu nào được xử lý thành công.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 171\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mĐã xử lý thành công \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_records_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bản ghi.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_ppg_segments:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKhông có dữ liệu nào được xử lý thành công.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Chuyển thành Numpy arrays\u001b[39;00m\n\u001b[0;32m    174\u001b[0m X_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_ppg_segments)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
            "\u001b[1;31mValueError\u001b[0m: Không có dữ liệu nào được xử lý thành công."
          ]
        }
      ],
      "source": [
        "# k\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "bidmc_combined_final_NO_NORM_CONDITIONS.ipynb\n",
        "\n",
        "Phiên bản kết hợp, nhưng KHÔNG chuẩn hóa điều kiện HR/RR.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt\n",
        "# import joblib # Không cần joblib nữa vì không lưu scaler\n",
        "\n",
        "# from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler # Chỉ cần MinMaxScaler cho PPG\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from scipy.signal import butter, filtfilt, welch, iirnotch\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Dữ liệu gốc lưu ở\n",
        "data_path = r'data/bidmc_data.mat'\n",
        "\n",
        "# Đường dẫn đến file dữ liệu (nên tạo thư mục mới cho phiên bản này)\n",
        "output_base_path = r'bidmc/combined_model_output_no_norm' # Thư mục mới\n",
        "processed_data_path = os.path.join(output_base_path, 'processed')\n",
        "figures_path = os.path.join(output_base_path, 'figures')\n",
        "results_path = os.path.join(output_base_path, 'results')\n",
        "model_path = os.path.join(output_base_path, 'models')\n",
        "\n",
        "# Tạo thư mục nếu chưa tồn tại\n",
        "os.makedirs(processed_data_path, exist_ok=True)\n",
        "os.makedirs(figures_path, exist_ok=True)\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "print(\"Thiết lập đường dẫn và thư mục hoàn tất.\")\n",
        "\n",
        "# --- Phần Explore Data (Giữ nguyên) ---\n",
        "print(\"Đang tải dữ liệu từ file .mat...\")\n",
        "try:\n",
        "    mat_data = sio.loadmat(data_path)\n",
        "    data = mat_data['data'][0]\n",
        "    print(f\"Số lượng bản ghi: {len(data)}\")\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải hoặc khám phá dữ liệu: {e}\")\n",
        "# --- Hết Phần Explore ---\n",
        "\n",
        "\n",
        "# --- Phần Preprocess Data ---\n",
        "print(\"\\nBắt đầu Tiền xử lý dữ liệu (KHÔNG chuẩn hóa HR/RR)...\")\n",
        "\n",
        "# Hàm chuẩn hóa tín hiệu PPG (MinMax về [0, 1])\n",
        "def normalize_signal_minmax(signal):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    signal_reshaped = signal.reshape(-1, 1)\n",
        "    normalized = scaler.fit_transform(signal_reshaped).flatten()\n",
        "    return normalized\n",
        "\n",
        "# Các hàm lọc (Giữ nguyên)\n",
        "def notch_filter(data, notch_freq=50.0, fs=125, quality_factor=30):\n",
        "    nyq = 0.5 * fs\n",
        "    w0 = notch_freq / nyq\n",
        "    if w0 >= 1.0:\n",
        "         print(f\"Warning: Notch frequency {notch_freq}Hz is too high for sampling rate {fs}Hz. Skipping notch filter.\")\n",
        "         return data\n",
        "    b, a = iirnotch(w0, quality_factor)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut=0.5, highcut=8.0, fs=125, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    low = max(0.01, low)\n",
        "    high = min(0.99, high)\n",
        "    if low >= high:\n",
        "        print(f\"Warning: Invalid frequency range [{lowcut}, {highcut}] for sampling rate {fs}. Skipping bandpass filter.\")\n",
        "        return data\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "# Hàm chia đoạn (Giữ nguyên)\n",
        "def segment_signal(signal, segment_length, overlap=0.5):\n",
        "    step = int(segment_length * (1 - overlap))\n",
        "    segments = []\n",
        "    if step <= 0: step = 1\n",
        "    for i in range(0, len(signal) - segment_length + 1, step):\n",
        "        segments.append(signal[i:i + segment_length])\n",
        "    if not segments:\n",
        "        print(f\"Warning: Could not create segments for signal of length {len(signal)} with segment_length {segment_length} and overlap {overlap}.\")\n",
        "    return np.array(segments)\n",
        "\n",
        "# Hàm trích xuất HR/RR trung bình (Giữ nguyên)\n",
        "def extract_mean_hr_rr(record):\n",
        "    hr_values_list = []\n",
        "    rr_values_list = []\n",
        "    try:\n",
        "        params = record['ref'][0, 0]['params'][0, 0]\n",
        "        hr_data = params['hr'][0]\n",
        "        rr_data = params['rr'][0]\n",
        "        # Xử lý HR\n",
        "        hr_values_raw = hr_data['v'] if hasattr(hr_data, 'dtype') and 'v' in hr_data.dtype.names else hr_data\n",
        "        for item in hr_values_raw:\n",
        "             val = item[0] if isinstance(item, (list, np.ndarray)) and len(item) > 0 else item\n",
        "             if np.isscalar(val) and not np.isnan(val): hr_values_list.append(float(val))\n",
        "        # Xử lý RR\n",
        "        rr_values_raw = rr_data['v'] if hasattr(rr_data, 'dtype') and 'v' in rr_data.dtype.names else rr_data\n",
        "        for item in rr_values_raw:\n",
        "            val = item[0] if isinstance(item, (list, np.ndarray)) and len(item) > 0 else item\n",
        "            if np.isscalar(val) and not np.isnan(val): rr_values_list.append(float(val))\n",
        "        if not hr_values_list or not rr_values_list: return None, None\n",
        "        return np.mean(hr_values_list), np.mean(rr_values_list)\n",
        "    except Exception as e: return None, None\n",
        "\n",
        "# Tham số tiền xử lý (Giữ nguyên)\n",
        "fs = 125\n",
        "segment_length = 8 * fs # 1000 samples\n",
        "overlap = 0.5\n",
        "lowcut = 0.5\n",
        "highcut = 8.0\n",
        "notch_freq = 50.0\n",
        "\n",
        "# Danh sách lưu trữ\n",
        "all_ppg_segments = []\n",
        "all_hr_conditions_raw = [] # Lưu giá trị gốc\n",
        "all_rr_conditions_raw = [] # Lưu giá trị gốc\n",
        "\n",
        "valid_records_count = 0\n",
        "# Vòng lặp xử lý từng bản ghi\n",
        "for i in range(len(data)):\n",
        "    try:\n",
        "        record = data[i]\n",
        "        ppg_signal_raw = record['ppg'][0, 0]['v'].flatten().astype(np.float64)\n",
        "        hr_mean, rr_mean = extract_mean_hr_rr(record)\n",
        "        if hr_mean is None or rr_mean is None:\n",
        "            # print(f\"Skipping record {i}: Insufficient HR/RR data.\") # Bỏ qua log này để đỡ rối\n",
        "            continue\n",
        "\n",
        "        ppg_notched = notch_filter(ppg_signal_raw, notch_freq=notch_freq, fs=fs)\n",
        "        ppg_filtered = butter_bandpass_filter(ppg_notched, lowcut=lowcut, highcut=highcut, fs=fs)\n",
        "        ppg_normalized = normalize_signal_minmax(ppg_filtered)\n",
        "        segments = segment_signal(ppg_normalized, segment_length, overlap)\n",
        "\n",
        "        if segments.size > 0:\n",
        "            num_segments = segments.shape[0]\n",
        "            all_ppg_segments.extend(segments)\n",
        "            all_hr_conditions_raw.extend([hr_mean] * num_segments) # Lưu giá trị gốc\n",
        "            all_rr_conditions_raw.extend([rr_mean] * num_segments) # Lưu giá trị gốc\n",
        "            valid_records_count += 1\n",
        "        # else: print(f\"Skipping record {i}: No segments generated after processing.\") # Bỏ qua log\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing record {i}: {e}\")\n",
        "\n",
        "print(f\"\\nĐã xử lý thành công {valid_records_count}/{len(data)} bản ghi.\")\n",
        "\n",
        "if not all_ppg_segments:\n",
        "    raise ValueError(\"Không có dữ liệu nào được xử lý thành công.\")\n",
        "\n",
        "# Chuyển thành Numpy arrays\n",
        "X_data = np.array(all_ppg_segments).astype(np.float32)\n",
        "# Giữ lại giá trị HR/RR gốc\n",
        "hr_data_raw = np.array(all_hr_conditions_raw).astype(np.float32).reshape(-1, 1)\n",
        "rr_data_raw = np.array(all_rr_conditions_raw).astype(np.float32).reshape(-1, 1)\n",
        "\n",
        "print(f\"Tổng số đoạn tín hiệu PPG: {X_data.shape[0]}\")\n",
        "\n",
        "# Chia train/test (90/10) - Sử dụng trực tiếp giá trị gốc\n",
        "X_train, X_test, hr_train, hr_test, rr_train, rr_test = train_test_split(\n",
        "    X_data, hr_data_raw, rr_data_raw, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Kích thước tập huấn luyện: {X_train.shape[0]}\")\n",
        "print(f\"Kích thước tập kiểm thử: {X_test.shape[0]}\")\n",
        "\n",
        "# ---> KHÔNG CẦN CHUẨN HÓA ĐIỀU KIỆN NỮA <---\n",
        "# print(\"\\nBỏ qua bước chuẩn hóa điều kiện HR và RR.\")\n",
        "\n",
        "# Tạo mảng điều kiện cuối cùng từ giá trị gốc\n",
        "condition_train = np.column_stack((hr_train.flatten(), rr_train.flatten())).astype(np.float32)\n",
        "condition_test = np.column_stack((hr_test.flatten(), rr_test.flatten())).astype(np.float32)\n",
        "\n",
        "print(\"Đã kết hợp điều kiện HR/RR (giá trị gốc).\")\n",
        "print(f\"Shape condition_train: {condition_train.shape}\")\n",
        "print(f\"Shape condition_test: {condition_test.shape}\")\n",
        "\n",
        "# Lưu dữ liệu đã xử lý\n",
        "np.save(os.path.join(processed_data_path, 'X_train.npy'), X_train)\n",
        "np.save(os.path.join(processed_data_path, 'X_test.npy'), X_test)\n",
        "np.save(os.path.join(processed_data_path, 'condition_train_raw.npy'), condition_train) # Lưu điều kiện gốc\n",
        "np.save(os.path.join(processed_data_path, 'condition_test_raw.npy'), condition_test)   # Lưu điều kiện gốc\n",
        "# Lưu thêm HR/RR riêng lẻ nếu cần\n",
        "np.save(os.path.join(processed_data_path, 'hr_train_raw.npy'), hr_train)\n",
        "np.save(os.path.join(processed_data_path, 'hr_test_raw.npy'), hr_test)\n",
        "np.save(os.path.join(processed_data_path, 'rr_train_raw.npy'), rr_train)\n",
        "np.save(os.path.join(processed_data_path, 'rr_test_raw.npy'), rr_test)\n",
        "print(\"Đã lưu các file dữ liệu đã xử lý vào:\", processed_data_path)\n",
        "\n",
        "print(\"\\nTiền xử lý dữ liệu hoàn tất (HR/RR không được chuẩn hóa).\")\n",
        "# --- Hết Phần Preprocess ---\n",
        "\n",
        "\n",
        "# --- Phần CVAE Model Definition (Giữ nguyên định nghĩa kiến trúc) ---\n",
        "print(\"\\nĐịnh nghĩa mô hình CVAE...\")\n",
        "\n",
        "# Tham số mô hình\n",
        "input_dim = 1000 # Giả định segment_length = 1000\n",
        "condition_dim = 2\n",
        "latent_dim = 32\n",
        "batch_size = 64\n",
        "epochs = 500\n",
        "beta_max = 1.5\n",
        "warmup_epochs = 50\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Lớp Sampling (Giữ nguyên)\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Encoder (Giữ nguyên)\n",
        "def build_encoder(input_dim, condition_dim, latent_dim, hidden_units_dense=[256, 128]):\n",
        "    encoder_inputs = layers.Input(shape=(input_dim,), name='encoder_input')\n",
        "    x = layers.Reshape((input_dim, 1))(encoder_inputs)\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5, strides=1, activation='swish', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=128, kernel_size=5, strides=1, activation='swish', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Conv1D(filters=256, kernel_size=5, strides=1, activation='swish', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    shape_before_flattening = tf.keras.backend.int_shape(x)[1:]\n",
        "    x = layers.Flatten()(x)\n",
        "    condition_inputs = layers.Input(shape=(condition_dim,), name='condition_input')\n",
        "    cond_dense = layers.Dense(16, activation='relu')(condition_inputs)\n",
        "    x = layers.Concatenate()([x, cond_dense])\n",
        "    for units in hidden_units_dense:\n",
        "        x = layers.Dense(units, activation='swish')(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "    encoder = Model([encoder_inputs, condition_inputs], [z_mean, z_log_var, z], name='encoder')\n",
        "    return encoder, shape_before_flattening\n",
        "\n",
        "# Decoder (Giữ nguyên, vẫn dùng sigmoid output để khớp PPG chuẩn hóa [0,1])\n",
        "def build_decoder(latent_dim, condition_dim, input_dim, shape_before_flattening, hidden_units_dense=[128, 256]):\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,), name='latent_input')\n",
        "    condition_inputs = layers.Input(shape=(condition_dim,), name='condition_input')\n",
        "    cond_dense = layers.Dense(16, activation='relu')(condition_inputs)\n",
        "    x = layers.Concatenate()([latent_inputs, cond_dense])\n",
        "    for units in reversed(hidden_units_dense):\n",
        "         x = layers.Dense(units, activation='swish')(x)\n",
        "    target_shape_units = np.prod(shape_before_flattening)\n",
        "    x = layers.Dense(target_shape_units, activation='swish')(x)\n",
        "    x = layers.Reshape(shape_before_flattening)(x)\n",
        "    x = layers.Conv1DTranspose(128, kernel_size=5, strides=2, padding='same', activation='swish')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv1DTranspose(64, kernel_size=5, strides=2, padding='same', activation='swish')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv1DTranspose(1, kernel_size=5, strides=2, padding='same', activation='sigmoid')(x) # Giữ sigmoid\n",
        "    decoder_outputs = layers.Reshape((input_dim,))(x)\n",
        "    decoder = Model([latent_inputs, condition_inputs], decoder_outputs, name='decoder')\n",
        "    return decoder\n",
        "\n",
        "# Lớp CVAE (Giữ nguyên logic KL Annealing)\n",
        "class CVAE_Combined(Model):\n",
        "    # ... (Giữ nguyên toàn bộ định nghĩa lớp CVAE_Combined như phiên bản trước) ...\n",
        "    def __init__(self, encoder, decoder, latent_dim, beta_max, warmup_epochs, **kwargs):\n",
        "        super(CVAE_Combined, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta_max = tf.constant(beta_max, dtype=tf.float32)\n",
        "        self.warmup_epochs = tf.constant(warmup_epochs, dtype=tf.float32)\n",
        "        self.epoch_counter = tf.Variable(0.0, trainable=False, name=\"epoch_counter\", dtype=tf.float32)\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "        self.beta_tracker = tf.keras.metrics.Mean(name=\"beta\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [ self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker, self.beta_tracker ]\n",
        "\n",
        "    def get_current_beta(self):\n",
        "         safe_warmup_epochs = tf.maximum(self.warmup_epochs, 1.0)\n",
        "         current_progress = (self.epoch_counter + 1.0) / safe_warmup_epochs\n",
        "         beta = tf.minimum(self.beta_max, self.beta_max * current_progress)\n",
        "         return beta\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, condition = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder([x, condition], training=True)\n",
        "            reconstruction = self.decoder([z, condition], training=True)\n",
        "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.mse(x, reconstruction), axis=1))\n",
        "            kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
        "            current_beta = self.get_current_beta()\n",
        "            total_loss = reconstruction_loss + current_beta * kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        self.beta_tracker.update_state(current_beta)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, condition = data\n",
        "        z_mean, z_log_var, z = self.encoder([x, condition], training=False)\n",
        "        reconstruction = self.decoder([z, condition], training=False)\n",
        "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.mse(x, reconstruction), axis=1))\n",
        "        kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
        "        current_beta = self.get_current_beta()\n",
        "        total_loss = reconstruction_loss + current_beta * kl_loss\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        self.beta_tracker.update_state(current_beta)\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def generate(self, condition, z=None, noise_scale=1.0):\n",
        "         if z is None: z = tf.random.normal(shape=(tf.shape(condition)[0], self.latent_dim)) * noise_scale\n",
        "         if tf.rank(condition) == 1: condition = tf.expand_dims(condition, axis=0)\n",
        "         return self.decoder([z, condition], training=False)\n",
        "\n",
        "# Callback cập nhật epoch (Giữ nguyên)\n",
        "class EpochCounterCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        if hasattr(self.model, 'epoch_counter'):\n",
        "            self.model.epoch_counter.assign(tf.cast(epoch, tf.float32))\n",
        "\n",
        "# Xây dựng mô hình\n",
        "print(\"\\nXây dựng Encoder và Decoder...\")\n",
        "encoder, shape_enc = build_encoder(input_dim, condition_dim, latent_dim)\n",
        "decoder = build_decoder(latent_dim, condition_dim, input_dim, shape_enc)\n",
        "# encoder.summary(line_length=100) # Bỏ summary để đỡ dài\n",
        "# decoder.summary(line_length=100)\n",
        "\n",
        "print(\"\\nXây dựng và biên dịch mô hình CVAE...\")\n",
        "cvae_model_no_norm = CVAE_Combined(encoder, decoder, latent_dim, beta_max, warmup_epochs)\n",
        "cvae_model_no_norm.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "print(\"Biên dịch hoàn tất.\")\n",
        "\n",
        "# --- Hết Phần Model Definition ---\n",
        "\n",
        "\n",
        "# --- Phần Training ---\n",
        "print(\"\\nBắt đầu Huấn luyện mô hình CVAE (HR/RR không chuẩn hóa)...\")\n",
        "\n",
        "# Load dữ liệu đã xử lý (X_train, condition_train_raw)\n",
        "X_train = np.load(os.path.join(processed_data_path, 'X_train.npy'))\n",
        "# LƯU Ý: Load condition_train_raw.npy chứa giá trị gốc\n",
        "condition_train = np.load(os.path.join(processed_data_path, 'condition_train_raw.npy'))\n",
        "# Load dữ liệu test tương ứng\n",
        "X_test = np.load(os.path.join(processed_data_path, 'X_test.npy'))\n",
        "condition_test = np.load(os.path.join(processed_data_path, 'condition_test_raw.npy'))\n",
        "\n",
        "\n",
        "# Thiết lập Callbacks (Giữ nguyên như phiên bản kết hợp)\n",
        "log_dir = os.path.join(model_path, \"logs_no_norm\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "checkpoint_path = os.path.join(model_path, \"cvae_no_norm_cond_best.weights.h5\") # Tên file mới\n",
        "checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, monitor='val_total_loss', mode='min', save_best_only=True, verbose=1)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_total_loss', patience=50, restore_best_weights=True, verbose=1)\n",
        "reduce_lr_callback = ReduceLROnPlateau(monitor='val_total_loss', factor=0.2, patience=20, min_lr=1e-7, verbose=1)\n",
        "combined_callbacks = [EpochCounterCallback(), tensorboard_callback, checkpoint_callback, early_stopping_callback, reduce_lr_callback]\n",
        "\n",
        "# Tạo dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, condition_train)).shuffle(buffer_size=X_train.shape[0]).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, condition_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Huấn luyện\n",
        "start_time = time.time()\n",
        "history = cvae_model_no_norm.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs,\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=combined_callbacks\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "print(f\"\\nHuấn luyện hoàn tất trong {training_time:.2f} giây.\")\n",
        "print(f\"Trọng số tốt nhất đã được lưu tại (hoặc khôi phục): {checkpoint_path}\")\n",
        "\n",
        "# Vẽ biểu đồ huấn luyện (Giữ nguyên)\n",
        "# ... (Code vẽ biểu đồ giống phiên bản trước) ...\n",
        "print('\\nVẽ biểu đồ quá trình huấn luyện...')\n",
        "plt.figure(figsize=(20, 5))\n",
        "plt.subplot(1, 5, 1); plt.plot(history.history['total_loss'], label='Train'); plt.plot(history.history['val_total_loss'], label='Val'); plt.title('Total Loss'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1, 5, 2); plt.plot(history.history['reconstruction_loss'], label='Train'); plt.plot(history.history['val_reconstruction_loss'], label='Val'); plt.title('Recon Loss'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1, 5, 3); plt.plot(history.history['kl_loss'], label='Train'); plt.plot(history.history['val_kl_loss'], label='Val'); plt.title('KL Loss'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1, 5, 4); beta_key = 'val_beta' if 'val_beta' in history.history else 'beta'; plt.plot(history.history.get(beta_key, []), label='Beta'); plt.title('Beta'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.subplot(1, 5, 5); plt.plot(history.history.get('lr', []), label='LR'); plt.title('Learning Rate'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout(); plt.savefig(os.path.join(figures_path, 'training_history_no_norm.png')); plt.show(); plt.close()\n",
        "print(\"Đã lưu biểu đồ huấn luyện.\")\n",
        "# --- Hết Phần Training ---\n",
        "\n",
        "\n",
        "# --- Phần Test & Evaluation ---\n",
        "print(\"\\nBắt đầu Kiểm tra và Đánh giá mô hình (HR/RR không chuẩn hóa)...\")\n",
        "\n",
        "# Load trọng số tốt nhất\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Đang tải trọng số tốt nhất từ: {checkpoint_path}\")\n",
        "    try:\n",
        "        _ = cvae_model_no_norm([X_test[:1], condition_test[:1]]) # Build nếu cần\n",
        "        cvae_model_no_norm.load_weights(checkpoint_path)\n",
        "        print(\"Đã tải trọng số thành công.\")\n",
        "    except Exception as e: print(f\"Lỗi khi tải trọng số: {e}.\")\n",
        "else: print(f\"Không tìm thấy file checkpoint: {checkpoint_path}.\")\n",
        "\n",
        "# Tái tạo tín hiệu từ tập Test (dùng condition_test gốc)\n",
        "num_samples_to_show = 10\n",
        "X_test_subset = X_test[:num_samples_to_show]\n",
        "condition_test_subset = condition_test[:num_samples_to_show] # Điều kiện gốc\n",
        "# Load HR/RR gốc để hiển thị\n",
        "hr_test_raw_subset = np.load(os.path.join(processed_data_path, 'hr_test_raw.npy'))[:num_samples_to_show]\n",
        "rr_test_raw_subset = np.load(os.path.join(processed_data_path, 'rr_test_raw.npy'))[:num_samples_to_show]\n",
        "\n",
        "print(\"Tái tạo tín hiệu từ tập test...\")\n",
        "z_mean, _, z = cvae_model_no_norm.encoder.predict([X_test_subset, condition_test_subset])\n",
        "reconstructed_ppg = cvae_model_no_norm.decoder.predict([z, condition_test_subset])\n",
        "\n",
        "# Vẽ so sánh tín hiệu gốc và tái tạo (Giữ nguyên code vẽ)\n",
        "plt.figure(figsize=(15, 4 * num_samples_to_show // 2))\n",
        "for i in range(num_samples_to_show):\n",
        "    plt.subplot(num_samples_to_show // 2, 2, i + 1)\n",
        "    plt.plot(X_test_subset[i], label='Original', alpha=0.8)\n",
        "    plt.plot(reconstructed_ppg[i], label='Reconstructed', alpha=0.8, linestyle='--')\n",
        "    plt.title(f'Test Sample {i+1} (HR={hr_test_raw_subset[i,0]:.1f}, RR={rr_test_raw_subset[i,0]:.1f})')\n",
        "    plt.xlabel('Sample'); plt.ylabel('Amplitude'); plt.legend(); plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout(); plt.savefig(os.path.join(figures_path, 'reconstruction_comparison_no_norm.png')); plt.show(); plt.close()\n",
        "\n",
        "\n",
        "# Phân tích tần số (Giữ nguyên code phân tích và vẽ)\n",
        "# ... (Code FFT và vẽ phổ tần số) ...\n",
        "print(\"Phân tích phổ tần số...\")\n",
        "def analyze_frequency_spectrum(signal, fs):\n",
        "    n = len(signal)\n",
        "    if n == 0: return np.array([]), np.array([])\n",
        "    yf = fft(signal)\n",
        "    xf = fftfreq(n, 1/fs)[:n//2]; yf_abs = 2.0/n * np.abs(yf[0:n//2])\n",
        "    return xf, yf_abs\n",
        "num_fft_samples = 5\n",
        "plt.figure(figsize=(15, 5 * num_fft_samples))\n",
        "for i in range(num_fft_samples):\n",
        "    xf_orig, yf_orig = analyze_frequency_spectrum(X_test_subset[i], fs)\n",
        "    xf_recon, yf_recon = analyze_frequency_spectrum(reconstructed_ppg[i], fs)\n",
        "    plt.subplot(num_fft_samples, 2, 2 * i + 1)\n",
        "    if xf_orig.size > 0: plt.plot(xf_orig, yf_orig); peaks_orig_idx=np.argsort(yf_orig)[-3:]; plt.plot(xf_orig[peaks_orig_idx], yf_orig[peaks_orig_idx], 'ro', label='Peaks'); [plt.text(xf_orig[idx], yf_orig[idx], f'{xf_orig[idx]:.2f}Hz', fontsize=9) for idx in peaks_orig_idx]\n",
        "    plt.title(f'Original FFT (Sample {i+1})'); plt.xlabel('Freq (Hz)'); plt.ylabel('Amp'); plt.xlim(0, 10); plt.grid(True, alpha=0.3); plt.legend()\n",
        "    plt.subplot(num_fft_samples, 2, 2 * i + 2)\n",
        "    if xf_recon.size > 0: plt.plot(xf_recon, yf_recon, color='orange'); peaks_recon_idx = np.argsort(yf_recon)[-3:]; plt.plot(xf_recon[peaks_recon_idx], yf_recon[peaks_recon_idx], 'ro', label='Peaks'); [plt.text(xf_recon[idx], yf_recon[idx], f'{xf_recon[idx]:.2f}Hz', fontsize=9) for idx in peaks_recon_idx]\n",
        "    plt.title(f'Reconstructed FFT (Sample {i+1})'); plt.xlabel('Freq (Hz)'); plt.ylabel('Amp'); plt.xlim(0, 10); plt.grid(True, alpha=0.3); plt.legend()\n",
        "plt.tight_layout(); plt.savefig(os.path.join(figures_path, 'fft_comparison_no_norm.png')); plt.show(); plt.close()\n",
        "\n",
        "# Tính toán Metrics (Giữ nguyên logic tính toán)\n",
        "# ... (Code tính MSE, PSNR, Corr và lưu summary) ...\n",
        "print(\"Tính toán các chỉ số đánh giá...\")\n",
        "mse_list, psnr_list, corr_list = [], [], []\n",
        "def calculate_psnr(original, generated, max_pixel=1.0): mse=mean_squared_error(original,generated); return float('inf') if mse==0 else 20*np.log10(max_pixel/np.sqrt(mse))\n",
        "def calculate_corr(original, generated): return 0.0 if np.std(original)==0 or np.std(generated)==0 else np.corrcoef(original, generated)[0,1]\n",
        "num_eval_samples = min(1000, X_test.shape[0]); X_eval = X_test[:num_eval_samples]; cond_eval = condition_test[:num_eval_samples]\n",
        "print(f\"Tái tạo {num_eval_samples} mẫu để đánh giá...\")\n",
        "_, _, eval_z = cvae_model_no_norm.encoder.predict([X_eval, cond_eval])\n",
        "eval_reconstructed = cvae_model_no_norm.decoder.predict([eval_z, cond_eval])\n",
        "for i in range(num_eval_samples): orig=X_eval[i]; recon=eval_reconstructed[i]; mse_list.append(mean_squared_error(orig, recon)); psnr_list.append(calculate_psnr(orig, recon)); corr_list.append(calculate_corr(orig, recon))\n",
        "corr_list_valid = [c for c in corr_list if not np.isnan(c)]\n",
        "results_summary = {\"MSE\":{\"Mean\":np.mean(mse_list),\"Std\":np.std(mse_list),\"Min\":np.min(mse_list),\"Max\":np.max(mse_list)}, \"PSNR\":{\"Mean\":np.mean(psnr_list),\"Std\":np.std(psnr_list),\"Min\":np.min(psnr_list),\"Max\":np.max(psnr_list)}, \"Correlation\":{\"Mean\":np.mean(corr_list_valid),\"Std\":np.std(corr_list_valid),\"Min\":np.min(corr_list_valid),\"Max\":np.max(corr_list_valid)}}\n",
        "print(\"\\n--- Kết quả Đánh giá Định lượng ---\"); [print(f\"{m}: Mean={s['Mean']:.4f} +/- {s['Std']:.4f} (Min={s['Min']:.4f}, Max={s['Max']:.4f})\") for m, s in results_summary.items()]\n",
        "results_file_path = os.path.join(results_path, \"evaluation_summary_no_norm.txt\");\n",
        "with open(results_file_path, \"w\") as f: f.write(\"KẾT QUẢ ĐÁNH GIÁ MÔ HÌNH CVAE (KHÔNG CHUẨN HÓA HR/RR)\\n=======================================\\n\\n\"); [f.write(f\"{m}:\\n  - Mean: {s['Mean']:.4f}\\n  - Std Dev: {s['Std']:.4f}\\n  - Min: {s['Min']:.4f}\\n  - Max: {s['Max']:.4f}\\n\\n\") for m, s in results_summary.items()]\n",
        "print(f\"Đã lưu kết quả đánh giá vào: {results_file_path}\")\n",
        "# --- Hết Phần Test & Evaluation ---\n",
        "\n",
        "\n",
        "# --- Phần Prediction (Sinh tín hiệu mới - Dùng HR/RR gốc) ---\n",
        "print(\"\\nBắt đầu Sinh tín hiệu PPG mới (Dùng HR/RR gốc)...\")\n",
        "\n",
        "# ---> KHÔNG CẦN LOAD SCALER NỮA <---\n",
        "# print(\"Bỏ qua bước load scaler.\")\n",
        "\n",
        "# Tạo lưới các giá trị HR, RR gốc mong muốn (Giữ nguyên)\n",
        "hr_raw_values = np.linspace(60, 120, 5)\n",
        "rr_raw_values = np.linspace(8, 20, 5)\n",
        "\n",
        "# ---> KHÔNG CẦN CHUẨN HÓA GIÁ TRỊ MỚI <---\n",
        "# print(\"Sử dụng trực tiếp giá trị HR/RR gốc.\")\n",
        "\n",
        "# Tạo các cặp điều kiện trực tiếp từ giá trị gốc\n",
        "conditions_pred_list_raw = []\n",
        "conditions_raw_list = [] # Vẫn lưu để hiển thị title\n",
        "for hr_r in hr_raw_values:\n",
        "    for rr_r in rr_raw_values:\n",
        "        conditions_pred_list_raw.append([hr_r, rr_r]) # Dùng giá trị gốc\n",
        "        conditions_raw_list.append([hr_r, rr_r])\n",
        "\n",
        "conditions_pred_tensor = tf.constant(conditions_pred_list_raw, dtype=tf.float32)\n",
        "conditions_raw_array = np.array(conditions_raw_list)\n",
        "\n",
        "print(f\"\\nSinh {conditions_pred_tensor.shape[0]} tín hiệu PPG với các điều kiện HR/RR gốc...\")\n",
        "# Sinh tín hiệu\n",
        "generated_ppg_specific = cvae_model_no_norm.generate(conditions_pred_tensor, noise_scale=0.7).numpy()\n",
        "\n",
        "# Vẽ tín hiệu đã tạo (Giữ nguyên code vẽ)\n",
        "num_pred_to_show = min(10, generated_ppg_specific.shape[0])\n",
        "indices_to_show = np.random.choice(generated_ppg_specific.shape[0], num_pred_to_show, replace=False)\n",
        "plt.figure(figsize=(15, 4 * num_pred_to_show // 2))\n",
        "plot_count = 1\n",
        "for idx in indices_to_show:\n",
        "     if plot_count > num_pred_to_show: break\n",
        "     plt.subplot(num_pred_to_show // 2, 2, plot_count)\n",
        "     plt.plot(generated_ppg_specific[idx])\n",
        "     hr_orig = conditions_raw_array[idx, 0]\n",
        "     rr_orig = conditions_raw_array[idx, 1]\n",
        "     plt.title(f'Generated PPG (Target HR={hr_orig:.1f}, RR={rr_orig:.1f})')\n",
        "     plt.xlabel('Sample'); plt.ylabel('Amplitude'); plt.grid(True, alpha=0.3)\n",
        "     plot_count += 1\n",
        "plt.tight_layout(); plt.savefig(os.path.join(figures_path, 'generated_signals_specific_conditions_no_norm.png')); plt.show(); plt.close()\n",
        "print(\"Đã lưu biểu đồ các tín hiệu sinh ra.\")\n",
        "\n",
        "# --- Hết Phần Prediction ---\n",
        "\n",
        "print(\"\\n--- QUÁ TRÌNH HOÀN TẤT (KHÔNG CHUẨN HÓA HR/RR) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# P 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ybgHUsmhmRVG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thiết lập đường dẫn và thư mục hoàn tất.\n",
            "Đang tải dữ liệu từ file .mat...\n",
            "Số lượng bản ghi: 53\n",
            "\n",
            "Bắt đầu Tiền xử lý dữ liệu...\n",
            "Skipping record 0: Insufficient HR/RR data.\n",
            "Skipping record 1: Insufficient HR/RR data.\n",
            "Skipping record 2: Insufficient HR/RR data.\n",
            "Skipping record 3: Insufficient HR/RR data.\n",
            "Skipping record 4: Insufficient HR/RR data.\n",
            "Skipping record 5: Insufficient HR/RR data.\n",
            "Skipping record 6: Insufficient HR/RR data.\n",
            "Skipping record 7: Insufficient HR/RR data.\n",
            "Skipping record 8: Insufficient HR/RR data.\n",
            "Skipping record 9: Insufficient HR/RR data.\n",
            "Skipping record 10: Insufficient HR/RR data.\n",
            "Skipping record 11: Insufficient HR/RR data.\n",
            "Skipping record 12: Insufficient HR/RR data.\n",
            "Skipping record 13: Insufficient HR/RR data.\n",
            "Skipping record 14: Insufficient HR/RR data.\n",
            "Skipping record 15: Insufficient HR/RR data.\n",
            "Skipping record 16: Insufficient HR/RR data.\n",
            "Skipping record 17: Insufficient HR/RR data.\n",
            "Skipping record 18: Insufficient HR/RR data.\n",
            "Skipping record 19: Insufficient HR/RR data.\n",
            "Skipping record 20: Insufficient HR/RR data.\n",
            "Skipping record 21: Insufficient HR/RR data.\n",
            "Skipping record 22: Insufficient HR/RR data.\n",
            "Skipping record 23: Insufficient HR/RR data.\n",
            "Skipping record 24: Insufficient HR/RR data.\n",
            "Skipping record 25: Insufficient HR/RR data.\n",
            "Skipping record 26: Insufficient HR/RR data.\n",
            "Skipping record 27: Insufficient HR/RR data.\n",
            "Skipping record 28: Insufficient HR/RR data.\n",
            "Skipping record 29: Insufficient HR/RR data.\n",
            "Skipping record 30: Insufficient HR/RR data.\n",
            "Skipping record 31: Insufficient HR/RR data.\n",
            "Skipping record 32: Insufficient HR/RR data.\n",
            "Skipping record 33: Insufficient HR/RR data.\n",
            "Skipping record 34: Insufficient HR/RR data.\n",
            "Skipping record 35: Insufficient HR/RR data.\n",
            "Skipping record 36: Insufficient HR/RR data.\n",
            "Skipping record 37: Insufficient HR/RR data.\n",
            "Skipping record 38: Insufficient HR/RR data.\n",
            "Skipping record 39: Insufficient HR/RR data.\n",
            "Skipping record 40: Insufficient HR/RR data.\n",
            "Skipping record 41: Insufficient HR/RR data.\n",
            "Skipping record 42: Insufficient HR/RR data.\n",
            "Skipping record 43: Insufficient HR/RR data.\n",
            "Skipping record 44: Insufficient HR/RR data.\n",
            "Skipping record 45: Insufficient HR/RR data.\n",
            "Skipping record 46: Insufficient HR/RR data.\n",
            "Skipping record 47: Insufficient HR/RR data.\n",
            "Skipping record 48: Insufficient HR/RR data.\n",
            "Skipping record 49: Insufficient HR/RR data.\n",
            "Skipping record 50: Insufficient HR/RR data.\n",
            "Skipping record 51: Insufficient HR/RR data.\n",
            "Skipping record 52: Insufficient HR/RR data.\n",
            "\n",
            "Đã xử lý thành công 0/53 bản ghi.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Không có dữ liệu nào được xử lý thành công. Vui lòng kiểm tra dữ liệu đầu vào và quá trình tiền xử lý.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 203\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mĐã xử lý thành công \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_records_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bản ghi.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_ppg_segments:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKhông có dữ liệu nào được xử lý thành công. Vui lòng kiểm tra dữ liệu đầu vào và quá trình tiền xử lý.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Chuyển thành Numpy arrays\u001b[39;00m\n\u001b[0;32m    206\u001b[0m X_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(all_ppg_segments)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
            "\u001b[1;31mValueError\u001b[0m: Không có dữ liệu nào được xử lý thành công. Vui lòng kiểm tra dữ liệu đầu vào và quá trình tiền xử lý."
          ]
        }
      ],
      "source": [
        "# code ket hop co chuan hoa BR HR.ipynb\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "bidmc_combined_final.ipynb\n",
        "\n",
        "Kết hợp các cải tiến và sửa lỗi từ các phiên bản trước.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.io as sio\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import pywt # Thư viện Wavelet\n",
        "import joblib # Lưu/tải scaler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler # Thêm StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from scipy.signal import butter, filtfilt, welch, iirnotch # Thêm iirnotch\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard # Import callbacks đầy đủ\n",
        "\n",
        "import datetime\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "\n",
        "\n",
        "# Dữ liệu gốc lưu ở\n",
        "data_path = r'data\\bidmc_data.mat'\n",
        "\n",
        "# Đường dẫn đến file dữ liệu (nên tạo thư mục mới cho phiên bản này)\n",
        "output_base_path = 'bidmc/combined_model_output'\n",
        "processed_data_path = os.path.join(output_base_path, 'processed')\n",
        "figures_path = os.path.join(output_base_path, 'figures')\n",
        "results_path = os.path.join(output_base_path, 'results')\n",
        "model_path = os.path.join(output_base_path, 'models')\n",
        "\n",
        "# Tạo thư mục nếu chưa tồn tại\n",
        "os.makedirs(processed_data_path, exist_ok=True)\n",
        "os.makedirs(figures_path, exist_ok=True)\n",
        "os.makedirs(results_path, exist_ok=True)\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "print(\"Thiết lập đường dẫn và thư mục hoàn tất.\")\n",
        "\n",
        "# --- Phần Explore Data (Giữ nguyên hoặc tùy chỉnh nếu cần) ---\n",
        "# (Code khám phá dữ liệu từ các phiên bản trước có thể đặt ở đây)\n",
        "print(\"Đang tải dữ liệu từ file .mat...\")\n",
        "try:\n",
        "    mat_data = sio.loadmat(data_path)\n",
        "    data = mat_data['data'][0]\n",
        "    print(f\"Số lượng bản ghi: {len(data)}\")\n",
        "    # (Thêm code khám phá chi tiết nếu muốn)\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải hoặc khám phá dữ liệu: {e}\")\n",
        "    # Có thể dừng script hoặc xử lý lỗi khác ở đây\n",
        "# --- Hết Phần Explore ---\n",
        "\n",
        "\n",
        "# --- Phần Preprocess Data ---\n",
        "print(\"\\nBắt đầu Tiền xử lý dữ liệu...\")\n",
        "\n",
        "# Hàm chuẩn hóa tín hiệu PPG (MinMax về [0, 1])\n",
        "def normalize_signal_minmax(signal):\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    signal_reshaped = signal.reshape(-1, 1)\n",
        "    normalized = scaler.fit_transform(signal_reshaped).flatten()\n",
        "    return normalized\n",
        "\n",
        "# Hàm lọc nhiễu\n",
        "def notch_filter(data, notch_freq=50.0, fs=125, quality_factor=30): # fs=125 theo dữ liệu BIDMC\n",
        "    nyq = 0.5 * fs\n",
        "    w0 = notch_freq / nyq\n",
        "    if w0 >= 1.0: # Tần số notch không được lớn hơn hoặc bằng Nyquist\n",
        "         print(f\"Warning: Notch frequency {notch_freq}Hz is too high for sampling rate {fs}Hz. Skipping notch filter.\")\n",
        "         return data\n",
        "    b, a = iirnotch(w0, quality_factor)\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut=0.5, highcut=8.0, fs=125, order=4): # Giảm order một chút\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    # Đảm bảo low < high và cả hai đều < 1.0\n",
        "    low = max(0.01, low) # Tránh tần số quá thấp\n",
        "    high = min(0.99, high) # Tránh tần số quá cao\n",
        "    if low >= high:\n",
        "        print(f\"Warning: Invalid frequency range [{lowcut}, {highcut}] for sampling rate {fs}. Skipping bandpass filter.\")\n",
        "        return data\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return filtfilt(b, a, data)\n",
        "\n",
        "# Hàm chia đoạn\n",
        "def segment_signal(signal, segment_length, overlap=0.5): # overlap=0.5 là mặc định tốt\n",
        "    step = int(segment_length * (1 - overlap))\n",
        "    segments = []\n",
        "    if step <= 0: # Tránh step = 0 nếu overlap >= 1\n",
        "        step = 1\n",
        "    for i in range(0, len(signal) - segment_length + 1, step):\n",
        "        segments.append(signal[i:i + segment_length])\n",
        "    # Kiểm tra xem có tạo được segment nào không\n",
        "    if not segments:\n",
        "        print(f\"Warning: Could not create segments for signal of length {len(signal)} with segment_length {segment_length} and overlap {overlap}.\")\n",
        "    return np.array(segments)\n",
        "\n",
        "# Hàm trích xuất đặc trưng HR/RR trung bình (Giữ nguyên logic cũ, xem xét cải tiến nếu có thể)\n",
        "def extract_mean_hr_rr(record):\n",
        "    hr_values_list = []\n",
        "    rr_values_list = []\n",
        "    try:\n",
        "        params = record['ref'][0, 0]['params'][0, 0]\n",
        "        hr_data = params['hr'][0]\n",
        "        rr_data = params['rr'][0]\n",
        "\n",
        "        # Xử lý HR\n",
        "        if hasattr(hr_data, 'dtype') and hr_data.dtype.names is not None and 'v' in hr_data.dtype.names:\n",
        "            hr_values_raw = hr_data['v']\n",
        "        else:\n",
        "            hr_values_raw = hr_data\n",
        "        for item in hr_values_raw:\n",
        "             # Xử lý mảng lồng nhau có thể rỗng hoặc chứa giá trị đơn lẻ\n",
        "             val = item[0] if isinstance(item, (list, np.ndarray)) and len(item) > 0 else item\n",
        "             if np.isscalar(val) and not np.isnan(val):\n",
        "                 hr_values_list.append(float(val))\n",
        "\n",
        "        # Xử lý RR\n",
        "        if hasattr(rr_data, 'dtype') and rr_data.dtype.names is not None and 'v' in rr_data.dtype.names:\n",
        "            rr_values_raw = rr_data['v']\n",
        "        else:\n",
        "            rr_values_raw = rr_data\n",
        "        for item in rr_values_raw:\n",
        "            val = item[0] if isinstance(item, (list, np.ndarray)) and len(item) > 0 else item\n",
        "            if np.isscalar(val) and not np.isnan(val):\n",
        "                 rr_values_list.append(float(val))\n",
        "\n",
        "        if not hr_values_list or not rr_values_list:\n",
        "            return None, None # Trả về None nếu không có đủ dữ liệu hợp lệ\n",
        "\n",
        "        return np.mean(hr_values_list), np.mean(rr_values_list)\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Minor error extracting HR/RR: {e}\") # Gỡ lỗi nếu cần\n",
        "        return None, None\n",
        "\n",
        "# Tham số tiền xử lý\n",
        "fs = 125\n",
        "segment_length = 8 * fs # 1000 samples\n",
        "overlap = 0.5\n",
        "lowcut = 0.5\n",
        "highcut = 8.0\n",
        "notch_freq = 50.0 # Tần số nhiễu điện lưới (có thể là 60Hz ở một số nơi)\n",
        "\n",
        "# Danh sách lưu trữ\n",
        "all_ppg_segments = []\n",
        "all_hr_conditions = []\n",
        "all_rr_conditions = []\n",
        "\n",
        "valid_records_count = 0\n",
        "# Vòng lặp xử lý từng bản ghi\n",
        "for i in range(len(data)):\n",
        "    try:\n",
        "        record = data[i]\n",
        "        # Lấy tín hiệu PPG\n",
        "        ppg_signal_raw = record['ppg'][0, 0]['v'].flatten().astype(np.float64) # Đảm bảo float64 cho lọc\n",
        "\n",
        "        # Lấy HR, RR trung bình cho bản ghi này\n",
        "        hr_mean, rr_mean = extract_mean_hr_rr(record)\n",
        "        if hr_mean is None or rr_mean is None:\n",
        "            print(f\"Skipping record {i}: Insufficient HR/RR data.\")\n",
        "            continue\n",
        "\n",
        "        # 1. Lọc Notch\n",
        "        ppg_notched = notch_filter(ppg_signal_raw, notch_freq=notch_freq, fs=fs)\n",
        "        # 2. Lọc Bandpass\n",
        "        ppg_filtered = butter_bandpass_filter(ppg_notched, lowcut=lowcut, highcut=highcut, fs=fs)\n",
        "        # 3. Chuẩn hóa MinMax về [0, 1]\n",
        "        ppg_normalized = normalize_signal_minmax(ppg_filtered)\n",
        "        # 4. Chia đoạn\n",
        "        segments = segment_signal(ppg_normalized, segment_length, overlap)\n",
        "\n",
        "        if segments.size > 0:\n",
        "            num_segments = segments.shape[0]\n",
        "            all_ppg_segments.extend(segments)\n",
        "            # Gán cùng HR/RR trung bình cho tất cả segment của bản ghi này\n",
        "            all_hr_conditions.extend([hr_mean] * num_segments)\n",
        "            all_rr_conditions.extend([rr_mean] * num_segments)\n",
        "            valid_records_count += 1\n",
        "        else:\n",
        "             print(f\"Skipping record {i}: No segments generated after processing.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing record {i}: {e}\")\n",
        "\n",
        "print(f\"\\nĐã xử lý thành công {valid_records_count}/{len(data)} bản ghi.\")\n",
        "\n",
        "if not all_ppg_segments:\n",
        "    raise ValueError(\"Không có dữ liệu nào được xử lý thành công. Vui lòng kiểm tra dữ liệu đầu vào và quá trình tiền xử lý.\")\n",
        "\n",
        "# Chuyển thành Numpy arrays\n",
        "X_data = np.array(all_ppg_segments).astype(np.float32)\n",
        "hr_data = np.array(all_hr_conditions).astype(np.float32).reshape(-1, 1) # Reshape cho Scaler\n",
        "rr_data = np.array(all_rr_conditions).astype(np.float32).reshape(-1, 1) # Reshape cho Scaler\n",
        "\n",
        "print(f\"Tổng số đoạn tín hiệu PPG: {X_data.shape[0]}\")\n",
        "print(f\"Kích thước segment PPG: {X_data.shape[1]}\")\n",
        "print(f\"Số lượng giá trị HR: {hr_data.shape[0]}\")\n",
        "print(f\"Số lượng giá trị RR: {rr_data.shape[0]}\")\n",
        "\n",
        "# Chia train/test (90/10)\n",
        "X_train, X_test, hr_train_raw, hr_test_raw, rr_train_raw, rr_test_raw = train_test_split(\n",
        "    X_data, hr_data, rr_data, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Kích thước tập huấn luyện: {X_train.shape[0]}\")\n",
        "print(f\"Kích thước tập kiểm thử: {X_test.shape[0]}\")\n",
        "\n",
        "# ---> CHUẨN HÓA ĐIỀU KIỆN HR/RR <---\n",
        "print(\"\\nChuẩn hóa điều kiện HR và RR...\")\n",
        "hr_scaler = StandardScaler()\n",
        "hr_train_scaled = hr_scaler.fit_transform(hr_train_raw)\n",
        "hr_test_scaled = hr_scaler.transform(hr_test_raw)\n",
        "joblib.dump(hr_scaler, os.path.join(processed_data_path, 'hr_scaler.gz')) # Lưu scaler HR\n",
        "print(\"Đã lưu HR scaler.\")\n",
        "\n",
        "rr_scaler = StandardScaler()\n",
        "rr_train_scaled = rr_scaler.fit_transform(rr_train_raw)\n",
        "rr_test_scaled = rr_scaler.transform(rr_test_raw)\n",
        "joblib.dump(rr_scaler, os.path.join(processed_data_path, 'rr_scaler.gz')) # Lưu scaler RR\n",
        "print(\"Đã lưu RR scaler.\")\n",
        "\n",
        "# Tạo mảng điều kiện cuối cùng đã chuẩn hóa\n",
        "condition_train = np.column_stack((hr_train_scaled.flatten(), rr_train_scaled.flatten())).astype(np.float32)\n",
        "condition_test = np.column_stack((hr_test_scaled.flatten(), rr_test_scaled.flatten())).astype(np.float32)\n",
        "\n",
        "print(\"Đã chuẩn hóa và kết hợp điều kiện HR/RR.\")\n",
        "print(f\"Shape condition_train: {condition_train.shape}\")\n",
        "print(f\"Shape condition_test: {condition_test.shape}\")\n",
        "\n",
        "# Lưu dữ liệu đã xử lý\n",
        "np.save(os.path.join(processed_data_path, 'X_train.npy'), X_train)\n",
        "np.save(os.path.join(processed_data_path, 'X_test.npy'), X_test)\n",
        "np.save(os.path.join(processed_data_path, 'condition_train.npy'), condition_train)\n",
        "np.save(os.path.join(processed_data_path, 'condition_test.npy'), condition_test)\n",
        "# Lưu cả giá trị gốc để tham khảo nếu cần\n",
        "np.save(os.path.join(processed_data_path, 'hr_train_raw.npy'), hr_train_raw)\n",
        "np.save(os.path.join(processed_data_path, 'hr_test_raw.npy'), hr_test_raw)\n",
        "np.save(os.path.join(processed_data_path, 'rr_train_raw.npy'), rr_train_raw)\n",
        "np.save(os.path.join(processed_data_path, 'rr_test_raw.npy'), rr_test_raw)\n",
        "print(\"Đã lưu các file dữ liệu đã xử lý vào:\", processed_data_path)\n",
        "\n",
        "# (Tùy chọn) Vẽ biểu đồ phân phối HR/RR gốc và các segment mẫu như trước\n",
        "# ... (code vẽ biểu đồ có thể thêm vào đây) ...\n",
        "\n",
        "print(\"\\nTiền xử lý dữ liệu hoàn tất.\")\n",
        "# --- Hết Phần Preprocess ---\n",
        "\n",
        "\n",
        "# --- Phần CVAE Model Definition ---\n",
        "print(\"\\nĐịnh nghĩa mô hình CVAE...\")\n",
        "\n",
        "# Tải dữ liệu (nếu cần chạy riêng section này)\n",
        "# X_train = np.load(os.path.join(processed_data_path, 'X_train.npy'))\n",
        "# X_test = np.load(os.path.join(processed_data_path, 'X_test.npy'))\n",
        "# condition_train = np.load(os.path.join(processed_data_path, 'condition_train.npy'))\n",
        "# condition_test = np.load(os.path.join(processed_data_path, 'condition_test.npy'))\n",
        "\n",
        "# Tham số mô hình (Giữ nguyên hoặc tinh chỉnh)\n",
        "input_dim = X_train.shape[1] # 1000\n",
        "condition_dim = 2\n",
        "latent_dim = 32\n",
        "# hidden_units_dense_encoder = [256, 128, 64] # Units cho lớp Dense trong Encoder\n",
        "# hidden_units_dense_decoder = [64, 128, 256] # Units cho lớp Dense trong Decoder\n",
        "# Tham số huấn luyện\n",
        "batch_size = 64\n",
        "epochs = 500 # Giảm số epoch tối đa, dựa vào EarlyStopping\n",
        "beta_max = 1.5 # Giảm beta max một chút\n",
        "warmup_epochs = 50 # Tăng thời gian warmup\n",
        "learning_rate = 0.0005 # Giữ LR này\n",
        "\n",
        "# Lớp Sampling (Giữ nguyên)\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Encoder (Kiến trúc Conv1D)\n",
        "def build_encoder(input_dim, condition_dim, latent_dim, hidden_units_dense=[256, 128]): # Giảm bớt lớp Dense\n",
        "    encoder_inputs = layers.Input(shape=(input_dim,), name='encoder_input')\n",
        "    x = layers.Reshape((input_dim, 1))(encoder_inputs)\n",
        "\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5, strides=1, activation='swish', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x) # Thêm BN\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x) # 1000 -> 500\n",
        "\n",
        "    x = layers.Conv1D(filters=128, kernel_size=5, strides=1, activation='swish', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x) # Thêm BN\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x) # 500 -> 250\n",
        "\n",
        "    x = layers.Conv1D(filters=256, kernel_size=5, strides=1, activation='swish', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x) # Thêm BN\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x) # 250 -> 125\n",
        "\n",
        "    shape_before_flattening = tf.keras.backend.int_shape(x)[1:] # (125, 256)\n",
        "    x = layers.Flatten()(x)\n",
        "\n",
        "    condition_inputs = layers.Input(shape=(condition_dim,), name='condition_input')\n",
        "    # Nhúng điều kiện vào không gian chiều cao hơn\n",
        "    cond_dense = layers.Dense(16, activation='relu')(condition_inputs) # Nhúng điều kiện\n",
        "\n",
        "    x = layers.Concatenate()([x, cond_dense]) # Ghép đặc trưng PPG và điều kiện đã nhúng\n",
        "\n",
        "    for units in hidden_units_dense:\n",
        "        x = layers.Dense(units, activation='swish')(x)\n",
        "        x = layers.Dropout(0.3)(x) # Thêm Dropout\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
        "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
        "    z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "    encoder = Model([encoder_inputs, condition_inputs], [z_mean, z_log_var, z], name='encoder')\n",
        "    # Trả về cả shape để dùng cho decoder\n",
        "    return encoder, shape_before_flattening\n",
        "\n",
        "# Decoder (Kiến trúc Conv1DTranspose)\n",
        "def build_decoder(latent_dim, condition_dim, input_dim, shape_before_flattening, hidden_units_dense=[128, 256]): # Giảm bớt lớp Dense\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,), name='latent_input')\n",
        "    condition_inputs = layers.Input(shape=(condition_dim,), name='condition_input')\n",
        "\n",
        "    # Nhúng điều kiện tương tự encoder\n",
        "    cond_dense = layers.Dense(16, activation='relu')(condition_inputs)\n",
        "\n",
        "    x = layers.Concatenate()([latent_inputs, cond_dense])\n",
        "\n",
        "    for units in reversed(hidden_units_dense):\n",
        "         x = layers.Dense(units, activation='swish')(x)\n",
        "         # Có thể thêm Dropout ở đây nếu cần\n",
        "\n",
        "    target_shape_units = np.prod(shape_before_flattening)\n",
        "    x = layers.Dense(target_shape_units, activation='swish')(x)\n",
        "    x = layers.Reshape(shape_before_flattening)(x) # Shape: (None, 125, 256)\n",
        "\n",
        "    x = layers.Conv1DTranspose(128, kernel_size=5, strides=2, padding='same', activation='swish')(x) # 125 -> 250\n",
        "    x = layers.BatchNormalization()(x) # Thêm BN\n",
        "\n",
        "    x = layers.Conv1DTranspose(64, kernel_size=5, strides=2, padding='same', activation='swish')(x) # 250 -> 500\n",
        "    x = layers.BatchNormalization()(x) # Thêm BN\n",
        "\n",
        "    # Lớp cuối cùng: filters=1, activation='sigmoid' (SỬA LỖI)\n",
        "    x = layers.Conv1DTranspose(1, kernel_size=5, strides=2, padding='same', activation='sigmoid')(x) # 500 -> 1000\n",
        "\n",
        "    decoder_outputs = layers.Reshape((input_dim,))(x)\n",
        "    decoder = Model([latent_inputs, condition_inputs], decoder_outputs, name='decoder')\n",
        "    return decoder\n",
        "\n",
        "# Lớp CVAE với KL Annealing (Cập nhật metric tính toán)\n",
        "class CVAE_Combined(Model):\n",
        "    def __init__(self, encoder, decoder, latent_dim, beta_max, warmup_epochs, **kwargs):\n",
        "        super(CVAE_Combined, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta_max = tf.constant(beta_max, dtype=tf.float32)\n",
        "        self.warmup_epochs = tf.constant(warmup_epochs, dtype=tf.float32)\n",
        "        self.epoch_counter = tf.Variable(0.0, trainable=False, name=\"epoch_counter\", dtype=tf.float32)\n",
        "\n",
        "        # Trackers\n",
        "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
        "        self.beta_tracker = tf.keras.metrics.Mean(name=\"beta\") # Theo dõi beta thực tế\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        # Đảm bảo trả về đúng các tracker đã định nghĩa\n",
        "        return [\n",
        "            self.total_loss_tracker,\n",
        "            self.reconstruction_loss_tracker,\n",
        "            self.kl_loss_tracker,\n",
        "            self.beta_tracker\n",
        "        ]\n",
        "\n",
        "    # Hàm tính beta dựa trên epoch counter\n",
        "    def get_current_beta(self):\n",
        "         # Chia tránh zero division\n",
        "         safe_warmup_epochs = tf.maximum(self.warmup_epochs, 1.0)\n",
        "         # epoch_counter tăng từ 0.0\n",
        "         current_progress = (self.epoch_counter + 1.0) / safe_warmup_epochs\n",
        "         beta = tf.minimum(self.beta_max, self.beta_max * current_progress)\n",
        "         return beta\n",
        "\n",
        "    def train_step(self, data):\n",
        "        x, condition = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder([x, condition], training=True)\n",
        "            reconstruction = self.decoder([z, condition], training=True)\n",
        "\n",
        "            # Reconstruction loss (MSE)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                 tf.reduce_sum(\n",
        "                     tf.keras.losses.mse(x, reconstruction), axis=1 # Sum trên chiều thời gian\n",
        "                 )\n",
        "            )\n",
        "            # KL loss\n",
        "            kl_loss = -0.5 * tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1 # Sum trên chiều latent\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Lấy beta hiện tại\n",
        "            current_beta = self.get_current_beta()\n",
        "            total_loss = reconstruction_loss + current_beta * kl_loss\n",
        "\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "\n",
        "        # Cập nhật trackers\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        self.beta_tracker.update_state(current_beta) # Cập nhật beta tracker\n",
        "\n",
        "        # Trả về dict các metric hiện tại\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        x, condition = data\n",
        "        z_mean, z_log_var, z = self.encoder([x, condition], training=False)\n",
        "        reconstruction = self.decoder([z, condition], training=False)\n",
        "\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "             tf.reduce_sum(tf.keras.losses.mse(x, reconstruction), axis=1)\n",
        "        )\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1)\n",
        "        )\n",
        "\n",
        "        current_beta = self.get_current_beta() # Dùng beta tương ứng epoch hiện tại\n",
        "        total_loss = reconstruction_loss + current_beta * kl_loss\n",
        "\n",
        "        # Cập nhật trackers (quan trọng cho validation metrics)\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        self.beta_tracker.update_state(current_beta)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    # Không cần hàm call riêng nếu train_step/test_step đã đủ\n",
        "    # def call(self, inputs, training=False): # Thêm training flag nếu cần BN/Dropout\n",
        "    #     x, condition = inputs\n",
        "    #     z_mean, z_log_var, z = self.encoder([x, condition], training=training)\n",
        "    #     reconstruction = self.decoder([z, condition], training=training)\n",
        "    #     return reconstruction\n",
        "\n",
        "    def generate(self, condition, z=None, noise_scale=1.0): # noise_scale=1.0 là chuẩn\n",
        "         if z is None:\n",
        "             z = tf.random.normal(shape=(tf.shape(condition)[0], self.latent_dim)) * noise_scale\n",
        "         # Cần đảm bảo condition có đúng shape (batch_size, condition_dim)\n",
        "         if tf.rank(condition) == 1:\n",
        "             condition = tf.expand_dims(condition, axis=0)\n",
        "         return self.decoder([z, condition], training=False) # Luôn dùng training=False khi generate\n",
        "\n",
        "# Callback để cập nhật epoch counter (QUAN TRỌNG cho beta annealing)\n",
        "class EpochCounterCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        # Cập nhật biến epoch_counter của mô hình\n",
        "        if hasattr(self.model, 'epoch_counter'):\n",
        "            self.model.epoch_counter.assign(tf.cast(epoch, tf.float32))\n",
        "            # tf.print(\"Epoch Counter Updated:\", self.model.epoch_counter) # Debug nếu cần\n",
        "\n",
        "# Xây dựng mô hình\n",
        "print(\"\\nXây dựng Encoder và Decoder...\")\n",
        "encoder, shape_enc = build_encoder(input_dim, condition_dim, latent_dim)\n",
        "decoder = build_decoder(latent_dim, condition_dim, input_dim, shape_enc)\n",
        "encoder.summary(line_length=100)\n",
        "decoder.summary(line_length=100)\n",
        "\n",
        "print(\"\\nXây dựng và biên dịch mô hình CVAE kết hợp...\")\n",
        "cvae_combined = CVAE_Combined(encoder, decoder, latent_dim, beta_max, warmup_epochs)\n",
        "cvae_combined.compile(optimizer=Adam(learning_rate=learning_rate))\n",
        "print(\"Biên dịch hoàn tất.\")\n",
        "\n",
        "# --- Hết Phần Model Definition ---\n",
        "\n",
        "# --- Phần Training ---\n",
        "print(\"\\nBắt đầu Huấn luyện mô hình CVAE kết hợp...\")\n",
        "\n",
        "# Thiết lập Callbacks (SỬ DỤNG DANH SÁCH ĐẦY ĐỦ)\n",
        "log_dir = os.path.join(model_path, \"logs_combined\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "checkpoint_path = os.path.join(model_path, \"cvae_combined_best.weights.h5\") # Đổi tên file checkpoint\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_path,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_total_loss', # Theo dõi val_total_loss\n",
        "    mode='min',\n",
        "    save_best_only=True, # Chỉ lưu model tốt nhất\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_total_loss',\n",
        "    patience=50,  # Tăng patience lên một chút nữa\n",
        "    restore_best_weights=True, # QUAN TRỌNG: Khôi phục trọng số tốt nhất khi dừng\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "reduce_lr_callback = ReduceLROnPlateau(\n",
        "    monitor='val_total_loss',\n",
        "    factor=0.2, # Giảm LR mạnh hơn\n",
        "    patience=20, # Kiên nhẫn hơn trước khi giảm LR\n",
        "    min_lr=1e-7, # LR tối thiểu\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# DANH SÁCH CALLBACKS ĐẦY ĐỦ SẼ ĐƯỢC SỬ DỤNG\n",
        "combined_callbacks = [\n",
        "    EpochCounterCallback(), # Cập nhật epoch cho beta annealing\n",
        "    tensorboard_callback,\n",
        "    checkpoint_callback,    # Lưu model tốt nhất\n",
        "    early_stopping_callback,# Dừng sớm và khôi phục trọng số tốt nhất\n",
        "    reduce_lr_callback     # Giảm learning rate\n",
        "]\n",
        "\n",
        "# Tạo dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, condition_train)).shuffle(buffer_size=X_train.shape[0]).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, condition_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Huấn luyện\n",
        "start_time = time.time()\n",
        "history = cvae_combined.fit(\n",
        "    train_dataset,\n",
        "    epochs=epochs, # Số epochs lớn, EarlyStopping sẽ kiểm soát\n",
        "    validation_data=test_dataset,\n",
        "    callbacks=combined_callbacks # <-- SỬ DỤNG CALLBACKS ĐẦY ĐỦ\n",
        ")\n",
        "training_time = time.time() - start_time\n",
        "\n",
        "print(f\"\\nHuấn luyện hoàn tất trong {training_time:.2f} giây.\")\n",
        "print(f\"Trọng số tốt nhất đã được lưu tại (hoặc khôi phục vào model): {checkpoint_path}\")\n",
        "\n",
        "# Lưu trọng số cuối cùng (tùy chọn, vì EarlyStopping đã khôi phục best weights)\n",
        "# cvae_combined.save_weights(os.path.join(model_path, 'cvae_combined_final.weights.h5'))\n",
        "\n",
        "# Vẽ biểu đồ huấn luyện\n",
        "print('\\nVẽ biểu đồ quá trình huấn luyện...')\n",
        "plt.figure(figsize=(20, 5)) # Rộng hơn để chứa 5 plot\n",
        "\n",
        "# Loss tổng thể\n",
        "plt.subplot(1, 5, 1)\n",
        "plt.plot(history.history['total_loss'], label='Train Total Loss')\n",
        "plt.plot(history.history['val_total_loss'], label='Val Total Loss')\n",
        "plt.title('Total Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss tái tạo\n",
        "plt.subplot(1, 5, 2)\n",
        "plt.plot(history.history['reconstruction_loss'], label='Train Recon Loss')\n",
        "plt.plot(history.history['val_reconstruction_loss'], label='Val Recon Loss')\n",
        "plt.title('Reconstruction Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss KL\n",
        "plt.subplot(1, 5, 3)\n",
        "plt.plot(history.history['kl_loss'], label='Train KL Loss')\n",
        "plt.plot(history.history['val_kl_loss'], label='Val KL Loss')\n",
        "plt.title('KL Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Beta\n",
        "plt.subplot(1, 5, 4)\n",
        "if 'beta' in history.history: # Kiểm tra xem beta có trong history không (phụ thuộc Keras version và cách metric được log)\n",
        "     # Lấy beta từ val_beta nếu có, hoặc train_beta\n",
        "     beta_key = 'val_beta' if 'val_beta' in history.history else 'beta'\n",
        "     plt.plot(history.history[beta_key], label='Beta Value', color='orange')\n",
        "     plt.title('Beta Annealing')\n",
        "     plt.xlabel('Epoch')\n",
        "     plt.ylabel('Beta')\n",
        "     plt.legend()\n",
        "     plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "     plt.title('Beta (Not Logged)')\n",
        "\n",
        "\n",
        "# Learning Rate\n",
        "plt.subplot(1, 5, 5)\n",
        "if 'lr' in history.history:\n",
        "    plt.plot(history.history['lr'], label='Learning Rate', color='purple')\n",
        "    plt.title('Learning Rate Schedule')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Learning Rate')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "else:\n",
        "     plt.title('LR (Not Logged)')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(figures_path, 'training_history_combined.png'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "print(\"Đã lưu biểu đồ huấn luyện.\")\n",
        "# --- Hết Phần Training ---\n",
        "\n",
        "\n",
        "# --- Phần Test & Evaluation ---\n",
        "print(\"\\nBắt đầu Kiểm tra và Đánh giá mô hình...\")\n",
        "\n",
        "# >>> QUAN TRỌNG: Load trọng số tốt nhất đã lưu <<<\n",
        "# Ngay cả khi EarlyStopping có restore_best_weights=True, việc load lại đảm bảo chắc chắn\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Đang tải trọng số tốt nhất từ: {checkpoint_path}\")\n",
        "    try:\n",
        "        # Cần build model trước khi load weights nếu chưa được build đầy đủ\n",
        "        # Thử build bằng cách gọi với dữ liệu mẫu\n",
        "        _ = cvae_combined([X_test[:1], condition_test[:1]])\n",
        "        cvae_combined.load_weights(checkpoint_path)\n",
        "        print(\"Đã tải trọng số thành công.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi tải trọng số: {e}. Sử dụng trọng số hiện tại trong model.\")\n",
        "else:\n",
        "    print(f\"Không tìm thấy file checkpoint: {checkpoint_path}. Sử dụng trọng số cuối cùng trong model.\")\n",
        "\n",
        "\n",
        "# 1. Tái tạo tín hiệu từ tập Test\n",
        "print(\"Tái tạo tín hiệu từ tập test...\")\n",
        "num_samples_to_show = 10\n",
        "X_test_subset = X_test[:num_samples_to_show]\n",
        "condition_test_subset = condition_test[:num_samples_to_show]\n",
        "\n",
        "# Lấy z_mean, z_log_var, z từ encoder\n",
        "z_mean, z_log_var, z = cvae_combined.encoder.predict([X_test_subset, condition_test_subset])\n",
        "# Tái tạo từ z (sử dụng z thay vì z_mean để xem xét cả phần ngẫu nhiên)\n",
        "reconstructed_ppg = cvae_combined.decoder.predict([z, condition_test_subset])\n",
        "# Hoặc tái tạo trực tiếp bằng cách gọi model (nếu có hàm call)\n",
        "# reconstructed_ppg = cvae_combined.predict([X_test_subset, condition_test_subset])\n",
        "\n",
        "# Load lại HR/RR gốc để hiển thị tiêu đề cho dễ hiểu\n",
        "hr_test_raw_subset = np.load(os.path.join(processed_data_path, 'hr_test_raw.npy'))[:num_samples_to_show]\n",
        "rr_test_raw_subset = np.load(os.path.join(processed_data_path, 'rr_test_raw.npy'))[:num_samples_to_show]\n",
        "\n",
        "plt.figure(figsize=(15, 4 * num_samples_to_show // 2))\n",
        "for i in range(num_samples_to_show):\n",
        "    plt.subplot(num_samples_to_show // 2, 2, i + 1)\n",
        "    plt.plot(X_test_subset[i], label='Original', alpha=0.8)\n",
        "    plt.plot(reconstructed_ppg[i], label='Reconstructed', alpha=0.8, linestyle='--')\n",
        "    plt.title(f'Test Sample {i+1} (HR={hr_test_raw_subset[i,0]:.1f}, RR={rr_test_raw_subset[i,0]:.1f})')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(figures_path, 'reconstruction_comparison.png'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# 2. Phân tích tần số so sánh\n",
        "print(\"Phân tích phổ tần số...\")\n",
        "def analyze_frequency_spectrum(signal, fs):\n",
        "    n = len(signal)\n",
        "    if n == 0: return np.array([]), np.array([])\n",
        "    yf = fft(signal)\n",
        "    xf = fftfreq(n, 1/fs)[:n//2]\n",
        "    yf_abs = 2.0/n * np.abs(yf[0:n//2])\n",
        "    return xf, yf_abs\n",
        "\n",
        "num_fft_samples = 5\n",
        "plt.figure(figsize=(15, 5 * num_fft_samples))\n",
        "for i in range(num_fft_samples):\n",
        "    # Original\n",
        "    xf_orig, yf_orig = analyze_frequency_spectrum(X_test_subset[i], fs)\n",
        "    # Reconstructed\n",
        "    xf_recon, yf_recon = analyze_frequency_spectrum(reconstructed_ppg[i], fs)\n",
        "\n",
        "    plt.subplot(num_fft_samples, 2, 2 * i + 1)\n",
        "    if xf_orig.size > 0:\n",
        "      plt.plot(xf_orig, yf_orig)\n",
        "      peaks_orig_idx = np.argsort(yf_orig)[-3:] # Top 3 peaks\n",
        "      plt.plot(xf_orig[peaks_orig_idx], yf_orig[peaks_orig_idx], 'ro', label='Peaks')\n",
        "      for idx in peaks_orig_idx:\n",
        "           plt.text(xf_orig[idx], yf_orig[idx], f'{xf_orig[idx]:.2f}Hz', fontsize=9)\n",
        "    plt.title(f'Original FFT (Sample {i+1})')\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.xlim(0, 10) # Giới hạn tần số\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    plt.subplot(num_fft_samples, 2, 2 * i + 2)\n",
        "    if xf_recon.size > 0:\n",
        "      plt.plot(xf_recon, yf_recon, color='orange')\n",
        "      peaks_recon_idx = np.argsort(yf_recon)[-3:] # Top 3 peaks\n",
        "      plt.plot(xf_recon[peaks_recon_idx], yf_recon[peaks_recon_idx], 'ro', label='Peaks')\n",
        "      for idx in peaks_recon_idx:\n",
        "           plt.text(xf_recon[idx], yf_recon[idx], f'{xf_recon[idx]:.2f}Hz', fontsize=9)\n",
        "    plt.title(f'Reconstructed FFT (Sample {i+1})')\n",
        "    plt.xlabel('Frequency (Hz)')\n",
        "    plt.ylabel('Amplitude')\n",
        "    plt.xlim(0, 10)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(figures_path, 'fft_comparison.png'))\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "# 3. Tính toán Metrics định lượng\n",
        "print(\"Tính toán các chỉ số đánh giá...\")\n",
        "mse_list, psnr_list, corr_list = [], [], []\n",
        "\n",
        "def calculate_psnr(original, generated, max_pixel=1.0): # Max pixel là 1.0 do MinMax về [0,1]\n",
        "    mse = mean_squared_error(original, generated)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "    return 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "\n",
        "def calculate_corr(original, generated):\n",
        "     # Xử lý trường hợp tín hiệu là hằng số (std=0)\n",
        "     if np.std(original) == 0 or np.std(generated) == 0:\n",
        "         return 0.0 if np.allclose(original, generated) else np.nan\n",
        "     return np.corrcoef(original, generated)[0, 1]\n",
        "\n",
        "\n",
        "# Tính toán trên toàn bộ tập test hoặc một phần lớn hơn\n",
        "num_eval_samples = min(1000, X_test.shape[0]) # Đánh giá trên 1000 mẫu hoặc toàn bộ tập test\n",
        "X_eval = X_test[:num_eval_samples]\n",
        "cond_eval = condition_test[:num_eval_samples]\n",
        "\n",
        "# Tái tạo cho tập đánh giá\n",
        "print(f\"Tái tạo {num_eval_samples} mẫu để đánh giá...\")\n",
        "eval_z_mean, _, eval_z = cvae_combined.encoder.predict([X_eval, cond_eval])\n",
        "eval_reconstructed = cvae_combined.decoder.predict([eval_z, cond_eval]) # Dùng z để có cả phần ngẫu nhiên\n",
        "\n",
        "for i in range(num_eval_samples):\n",
        "    orig = X_eval[i]\n",
        "    recon = eval_reconstructed[i]\n",
        "    mse_list.append(mean_squared_error(orig, recon))\n",
        "    psnr_list.append(calculate_psnr(orig, recon))\n",
        "    corr_list.append(calculate_corr(orig, recon))\n",
        "\n",
        "# Loại bỏ NaN trong corr nếu có\n",
        "corr_list_valid = [c for c in corr_list if not np.isnan(c)]\n",
        "\n",
        "results_summary = {\n",
        "    \"MSE\": {\"Mean\": np.mean(mse_list), \"Std\": np.std(mse_list), \"Min\": np.min(mse_list), \"Max\": np.max(mse_list)},\n",
        "    \"PSNR\": {\"Mean\": np.mean(psnr_list), \"Std\": np.std(psnr_list), \"Min\": np.min(psnr_list), \"Max\": np.max(psnr_list)},\n",
        "    \"Correlation\": {\"Mean\": np.mean(corr_list_valid), \"Std\": np.std(corr_list_valid), \"Min\": np.min(corr_list_valid), \"Max\": np.max(corr_list_valid)}\n",
        "}\n",
        "\n",
        "print(\"\\n--- Kết quả Đánh giá Định lượng ---\")\n",
        "for metric, stats in results_summary.items():\n",
        "     print(f\"{metric}: Mean={stats['Mean']:.4f} +/- {stats['Std']:.4f} (Min={stats['Min']:.4f}, Max={stats['Max']:.4f})\")\n",
        "\n",
        "# Lưu kết quả vào file\n",
        "results_file_path = os.path.join(results_path, \"evaluation_summary.txt\")\n",
        "with open(results_file_path, \"w\") as f:\n",
        "    f.write(\"KẾT QUẢ ĐÁNH GIÁ MÔ HÌNH CVAE KẾT HỢP\\n\")\n",
        "    f.write(\"=======================================\\n\\n\")\n",
        "    f.write(f\"Đánh giá trên {num_eval_samples} mẫu.\\n\\n\")\n",
        "    for metric, stats in results_summary.items():\n",
        "        f.write(f\"{metric}:\\n\")\n",
        "        f.write(f\"  - Mean: {stats['Mean']:.4f}\\n\")\n",
        "        f.write(f\"  - Std Dev: {stats['Std']:.4f}\\n\")\n",
        "        f.write(f\"  - Min: {stats['Min']:.4f}\\n\")\n",
        "        f.write(f\"  - Max: {stats['Max']:.4f}\\n\\n\")\n",
        "print(f\"Đã lưu kết quả đánh giá vào: {results_file_path}\")\n",
        "\n",
        "# --- Hết Phần Test & Evaluation ---\n",
        "\n",
        "\n",
        "# --- Phần Prediction (Sinh tín hiệu mới) ---\n",
        "print(\"\\nBắt đầu Sinh tín hiệu PPG mới...\")\n",
        "\n",
        "# Load lại scaler đã lưu trong quá trình tiền xử lý\n",
        "try:\n",
        "    hr_scaler_loaded = joblib.load(os.path.join(processed_data_path, 'hr_scaler.gz'))\n",
        "    rr_scaler_loaded = joblib.load(os.path.join(processed_data_path, 'rr_scaler.gz'))\n",
        "    print(\"Đã tải thành công HR và RR scalers.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Lỗi: Không tìm thấy file scaler đã lưu. Không thể thực hiện dự đoán với giá trị gốc.\")\n",
        "    # Thoát hoặc xử lý lỗi khác\n",
        "    hr_scaler_loaded = None\n",
        "    rr_scaler_loaded = None\n",
        "\n",
        "if hr_scaler_loaded and rr_scaler_loaded:\n",
        "    # Tạo lưới các giá trị HR, RR gốc mong muốn\n",
        "    hr_raw_values = np.linspace(60, 120, 5) # Ví dụ: 60, 75, 90, 105, 120 bpm\n",
        "    rr_raw_values = np.linspace(8, 20, 5) # Ví dụ: 8, 11, 14, 17, 20 breaths/min\n",
        "\n",
        "    # Chuẩn hóa các giá trị này bằng scaler đã load\n",
        "    hr_scaled_pred = hr_scaler_loaded.transform(hr_raw_values.reshape(-1, 1))\n",
        "    rr_scaled_pred = rr_scaler_loaded.transform(rr_raw_values.reshape(-1, 1))\n",
        "\n",
        "    # Tạo các cặp điều kiện đã chuẩn hóa\n",
        "    conditions_pred_list = []\n",
        "    conditions_raw_list = [] # Lưu lại giá trị gốc để hiển thị\n",
        "    for hr_s, hr_r in zip(hr_scaled_pred, hr_raw_values):\n",
        "        for rr_s, rr_r in zip(rr_scaled_pred, rr_raw_values):\n",
        "            conditions_pred_list.append([hr_s[0], rr_s[0]])\n",
        "            conditions_raw_list.append([hr_r, rr_r])\n",
        "\n",
        "    conditions_pred_tensor = tf.constant(conditions_pred_list, dtype=tf.float32)\n",
        "    conditions_raw_array = np.array(conditions_raw_list)\n",
        "\n",
        "    print(f\"\\nSinh {conditions_pred_tensor.shape[0]} tín hiệu PPG với các điều kiện HR/RR cụ thể (đã chuẩn hóa)...\")\n",
        "    # Sinh tín hiệu (có thể thêm nhiễu z để đa dạng hóa)\n",
        "    generated_ppg_specific = cvae_combined.generate(conditions_pred_tensor, noise_scale=0.7).numpy() # Thêm chút nhiễu\n",
        "\n",
        "    # Vẽ một số tín hiệu đã tạo\n",
        "    num_pred_to_show = min(10, generated_ppg_specific.shape[0])\n",
        "    indices_to_show = np.random.choice(generated_ppg_specific.shape[0], num_pred_to_show, replace=False)\n",
        "\n",
        "    plt.figure(figsize=(15, 4 * num_pred_to_show // 2))\n",
        "    plot_count = 1\n",
        "    for idx in indices_to_show:\n",
        "         if plot_count > num_pred_to_show: break\n",
        "         plt.subplot(num_pred_to_show // 2, 2, plot_count)\n",
        "         plt.plot(generated_ppg_specific[idx])\n",
        "         hr_orig = conditions_raw_array[idx, 0]\n",
        "         rr_orig = conditions_raw_array[idx, 1]\n",
        "         plt.title(f'Generated PPG (Target HR={hr_orig:.1f}, RR={rr_orig:.1f})')\n",
        "         plt.xlabel('Sample')\n",
        "         plt.ylabel('Amplitude')\n",
        "         plt.grid(True, alpha=0.3)\n",
        "         plot_count += 1\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(figures_path, 'generated_signals_specific_conditions.png'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "    print(\"Đã lưu biểu đồ các tín hiệu sinh ra.\")\n",
        "\n",
        "# --- Hết Phần Prediction ---\n",
        "\n",
        "print(\"\\n--- QUÁ TRÌNH HOÀN TẤT ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aiot_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
